{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "This script makes use of open source datasets published to Hugging Face. In particular this will download various articles or writing samples with some associated questions and answers. This is great for experimenting with an FAQ RAG application, comparing new questions from users with existing questions in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, specify the dataset to download from Hugging Face (stanfordnlp/coqa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "REPO_ID = \"stanfordnlp/coqa\"\n",
    "\n",
    "dataset = load_dataset(REPO_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tables in the Vector Database to store the raw and embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intersystems_iris.dbapi._DBAPI as iris\n",
    "\n",
    "conn = iris.connect(hostname='localhost', \n",
    "                    port=51972, \n",
    "                    namespace='USER',\n",
    "                    username='SuperUser', \n",
    "                    password='SYS')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "                CREATE TABLE RAG_COQA.Story (\n",
    "                    StoryId INTEGER,\n",
    "                    Source VARCHAR(50),\n",
    "                    Story VARCHAR(10000),\n",
    "                    StoryEmbedding VECTOR(DOUBLE, 768)\n",
    "                )\n",
    "            \"\"\")\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "                CREATE TABLE RAG_COQA.QandA (\n",
    "                    StoryId INTEGER,\n",
    "                    Question VARCHAR(500),\n",
    "                    QuestionEmbedding VECTOR(DOUBLE, 768),\n",
    "                    Answer VARCHAR(1000)         \n",
    "                )\n",
    "            \"\"\")\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache the embedding model from Hugging Face locally. You can view a leaderboard of text embedding models based on the MTE Benchmark here: https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"avsolatorio/GIST-Embedding-v0\", cache_folder='.\\\\huggingface_cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract the 'train' data partition for the embeddings. To check the naming of the dataset splits, check the Hugging Face Dataset repo directly: https://huggingface.co/datasets/stanfordnlp/coqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = dataset['train']\n",
    "\n",
    "from itertools import islice\n",
    "iterator = islice(data_split, 10) # Slice top 10 stories\n",
    "for story in iterator:\n",
    "    print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embeddings from the partition, and persist them over SQL using our vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_split)\n",
    "print(data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only embed the first 300 articles, with all of their respective Q and As (4474 total). The embedding and persistence script may take a few minutes to complete. Check the progress bar to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = []\n",
    "\n",
    "story_insert = \"INSERT INTO RAG_COQA.Story (StoryId, Source, Story, StoryEmbedding) VALUES (?,?,?,?)\"\n",
    "qanda_insert = \"INSERT INTO RAG_COQA.QandA (StoryId, Question, QuestionEmbedding, Answer) VALUES (?,?,TO_VECTOR(?,DOUBLE,768),?)\"\n",
    "\n",
    "story_cursor = conn.cursor()\n",
    "qanda_cursor = conn.cursor()\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "limit = 300\n",
    "assert limit < len(data_split), \"Limit cannot surpass size of data set.\"\n",
    "\n",
    "with alive_bar(limit, force_tty=True) as bar:\n",
    "    qanda_count = 0\n",
    "    for idx, story in enumerate(data_split, start=1):\n",
    "        #;\n",
    "        story_embedding = model.encode(story['story'])\n",
    "        story_embedding_to_list = str(story_embedding.tolist())[1:-1]\n",
    "        story_cursor.execute(story_insert, [idx, story['source'][0:9990], story['story'], story_embedding_to_list])\n",
    "        #;\n",
    "        # Create embeddings for the questions with the pre-trained model.\n",
    "        chunk = []\n",
    "        questions = list(story['questions']) # This is a list[str]\n",
    "        answers = list(story['answers']['input_text']) # This is also a list[str]\n",
    "        qanda_count += len(questions)\n",
    "        question_embeddings = model.encode(questions) # This is a list[tensor]\n",
    "        #;\n",
    "        #\n",
    "        question_embeddings_to_list = [str(embedding.tolist())[1:-1] for embedding in question_embeddings]\n",
    "        #;\n",
    "        #\n",
    "        for qdx, question in enumerate(questions):\n",
    "            chunk.append([idx, question, question_embeddings_to_list[qdx], answers[qdx][0:999]])\n",
    "        qanda_cursor.executemany(qanda_insert, chunk)\n",
    "        #\n",
    "        if idx == limit + 1: \n",
    "            break\n",
    "        bar()\n",
    "\n",
    "print(\"Questions embedded: \" + str(qanda_count), end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the tables, run a DROP command. Dropping these tables will lose all embeddings data from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop tables\n",
    "drop = input(\"Warning! Are you sure you want to drop tables? You will need to rebuild the tables and embeddings to run any further exercises. (Y/N)\")\n",
    "if drop == \"Y\":\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE RAG_COQA.Story\")\n",
    "    cursor.execute(\"DROP TABLE RAG_COQA.QandA\")\n",
    "    cursor.close()\n",
    "    print(\"Tables dropped.\")\n",
    "else:\n",
    "    print(\"Tables not dropped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
