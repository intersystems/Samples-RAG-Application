{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought\n",
    "This technique requires the language model to describe its solution in logical steps, and is particularly effective at improving answers to mathematical or reasoning questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialize the LLM object with your provider of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\SRC\\Non-PoC\\rag-learning-services\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"KEY-ABC-DEF\",\n",
    "    temperature = 0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows a chain of thought prompt being used with few-shot, to illicit a similar output from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cafeteria started with 23 apples. They used 20 for lunch, leaving 3 apples. They then bought 6 more apples, bringing the total to 3 + 6 = 9 apples. The cafeteria now has 9 apples.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
    "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Q: The cafeteria had 23 apples.\n",
    "If they used 20 to make lunch and bought 6 more, how many tennis balls do they have?\n",
    "A:\"\"\"\n",
    "\n",
    "llm.predict(example + '\\n' + question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example further explores \"Tree of thought\", which combines Chain of Thought with a \"Panel of Experts\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert 1: The ball is in the cup when Bob puts it in the bedroom.\n",
      "Expert 2: No, the ball falls out of the cup when Bob turns it upside down in the bedroom.\n",
      "Expert 3: Actually, the ball is left in the cup when Bob puts it down in the garden.\n"
     ]
    }
   ],
   "source": [
    "# Based off https://github.com/dave1010/tree-of-thought-prompting\n",
    "\n",
    "situation=\"\"\"\n",
    "Bob is in the living room.\n",
    "He walks to the kitchen, carrying a cup.\n",
    "He puts a ball in the cup and carries the cup to the bedroom.\n",
    "He turns the cup upside down, then walks to the garden.\n",
    "He puts the cup down in the garden, then walks to the garage.\n",
    "Where is the ball?\n",
    "\"\"\"\n",
    "\n",
    "tree_of_thought=\"\"\"\n",
    "Imagine three different experts are answering this question.\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "Then all experts will go on to the next step, etc.\n",
    "If any expert realises they're wrong at any point then they leave.\n",
    "The question is...\n",
    "\"\"\"\n",
    "\n",
    "print(llm.predict(tree_of_thought + '\\n' + situation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Chain of Thought with vectors in our database as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_app.vector_search import VectorSearch\n",
    "\n",
    "# Initiate the database connection\n",
    "vector_database = VectorSearch(host='localhost', port=51972, namespace='USER', username='SuperUser', password='sys')\n",
    "\n",
    "# Load the cached text embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"avsolatorio/GIST-Embedding-v0\", cache_folder='.\\\\huggingface_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a useful assistant. \n",
      "These are examples of how you should answer your user's question by thinking through your steps very carefully: \n",
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
      "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
      "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
      "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "\n",
      "\n",
      "This is their question which you should answer, making sure you explain your steps carefully: \n",
      "How many manuscripts would the Vatican Library need to digitise over its 4 year digitisation project?\n",
      "\n",
      "These are supporting documents that you should use to form your answer to the user, which you can use in your steps: \n",
      "[['The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula. \\n\\nThe Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail. \\n\\nIn March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online. \\n\\nThe Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items. \\n\\nScholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican. \\n\\nThe Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.', 1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = \"How many manuscripts would the Vatican Library need to digitise over its 4 year digitisation project?\"\n",
    "\n",
    "embedding = model.encode(input)\n",
    "\n",
    "similar_results = vector_database.search_vector_db_with_embedding(embedding.tolist(), top_k=1)\n",
    "document = similar_results\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a useful assistant. \n",
    "These are examples of how you should answer your user's question by thinking through your steps very carefully: \n",
    "{example}\n",
    "\n",
    "This is their question which you should answer, making sure you explain your steps carefully: \n",
    "{input}\n",
    "\n",
    "These are supporting documents that you should use to form your answer to the user, which you can use in your steps: \n",
    "{document}\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To calculate how many manuscripts the Vatican Library would need to digitize over its 4-year digitization project, we need to consider the information provided in the supporting document.\\n\\nThe Vatican Library began a 4-year project in March 2014 to digitize its collection of manuscripts. The library has a significant collection of historical texts, including 75,000 codices and 1.1 million printed books.\\n\\nTo determine the number of manuscripts that would need to be digitized, we need to focus on the 75,000 codices, as these are handwritten manuscripts. Therefore, the Vatican Library would need to digitize 75,000 codices over the course of the 4-year project.\\n\\nTherefore, the Vatican Library would need to digitize 75,000 manuscripts over its 4-year digitization project.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
