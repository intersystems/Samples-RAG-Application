{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought\n",
    "This technique requires the language model to describe its solution in logical steps, and is particularly effective at improving answers to mathematical or reasoning questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialize the LLM object with your provider of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"KEY-ABC-DEF\",\n",
    "    temperature = 0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows a chain of thought prompt being used with few-shot, to illicit a similar output from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls.\n",
    "Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
    "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Q: The cafeteria had 23 apples.\n",
    "If they used 20 to make lunch and bought 6 more, how many tennis balls do they have?\n",
    "A:\"\"\"\n",
    "\n",
    "llm.predict(example + '\\n' + question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example further explores \"Tree of thought\", which combines Chain of Thought with a \"Panel of Experts\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off https://github.com/dave1010/tree-of-thought-prompting\n",
    "\n",
    "situation=\"\"\"\n",
    "Bob is in the living room.\n",
    "He walks to the kitchen, carrying a cup.\n",
    "He puts a ball in the cup and carries the cup to the bedroom.\n",
    "He turns the cup upside down, then walks to the garden.\n",
    "He puts the cup down in the garden, then walks to the garage.\n",
    "Where is the ball?\n",
    "\"\"\"\n",
    "\n",
    "tree_of_thought=\"\"\"\n",
    "Imagine three different experts are answering this question.\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "Then all experts will go on to the next step, etc.\n",
    "If any expert realises they're wrong at any point then they leave.\n",
    "The question is...\n",
    "\"\"\"\n",
    "\n",
    "print(llm.predict(tree_of_thought + '\\n' + situation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Chain of Thought with vectors in our database as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_app.vector_search import VectorSearch\n",
    "\n",
    "# Initiate the database connection\n",
    "vector_database = VectorSearch(host='localhost', port=51972, namespace='USER', username='SuperUser', password='SYS')\n",
    "\n",
    "# Load the cached text embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"avsolatorio/GIST-Embedding-v0\", cache_folder='.\\\\huggingface_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"How many manuscripts would the Vatican Library need to digitise over its 4 year digitisation project?\"\n",
    "\n",
    "embedding = model.encode(input)\n",
    "\n",
    "similar_results = vector_database.search_by_story(embedding.tolist(), top_k=1)\n",
    "document = similar_results\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a useful assistant. \n",
    "These are examples of how you should answer your user's question by thinking through your steps very carefully: \n",
    "{example}\n",
    "\n",
    "This is their question which you should answer, making sure you explain your steps carefully: \n",
    "{input}\n",
    "\n",
    "These are supporting documents that you should use to form your answer to the user, which you can use in your steps: \n",
    "{document}\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.predict(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
